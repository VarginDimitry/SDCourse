# Проектирование системы "Видео-стриминга"

## 1. Требования к системе:

### 1.1. Функциональные требования
1. Регистрация, авторизация.
2. Изменение профиля.
2. Загрузка, просмотр и удаление видео.
3. Список видео по фильтрам.
4. Лайк/дизлайк на видео.
5. Комментирование видео.
6. Просмотр статистики видео.

### 1.2. Нефункциональные требования
1. DAU - 2-3 млн.
2. MAU - 10 млн.
3. Хранение видео:
    * Разные виды качества кусочков видео (в том числе 2к и больше) + оригинальный файл - 3 месяца.
    * После 3го месяца начинаем убирать непопулярные форматы, оставляем только (480, 1080, 2к) и оригинальный файл.
    * через год оставляем только 1080 и оригинальный файл; если он большой, режем качество до 1080.
4. 95% HTTP запросов должны быть обработаны за <= 1 секунду.
5. Поддержка 1000 одновременных загрузок видео.
6. Задержка перед воспроизведением видео < 5 секунд (независимо от длительности видео).
7. Доступность системы >= 99,99 %/мес.
8. Все данные передаются по SSL/TLS.
9. Персональные данные в БД зашифрованы.
10. Автоматическое горизонтальное масштабирование (авто скейлинг).
11. Адаптивный интерфейс (mobile first).
12. Покрытие авто тестами >= 80%.

### 1.3. Целевые метрики
1. Время исполнения запросов (95% HTTP запросов должны быть обработаны за <= 1 секунду).
2. Количество одновременных загрузок (от автора на сервер) видео (Поддержка 1000 одновременных загрузок видео).
3. Скорость загрузки видео зрителем (Задержка перед воспроизведением видео < 5 секунд).
4. Доступность системы (>= 99,99 %/мес).

Т.е. по CAP теореме выбираем как приоритет Availability и Partition tolerance.


## 2. Архитектурное проектирование

### 2.1. Схема:
![Архитектурная схема](./system-diagram.jpg)

### 2.2. Ключевые компоненты:
1. Балансировщики нагрузки / Nginx.
2. API Gateway.
3. Набор сервисов для обработки видео.
    1. Сервис для получения первичных данных о видео.
    2. Временное хранилище для сырого видео.
    3. Очередь.
    4. Сервис обработки видео.
4. Сервис данных / статистики о видео.
5. Сервис валидации комментариев.
6. Сервис авторизации
7. Сервис пользовательских данных (для снятия нагрузки с сервиса авторизации).
8. Сервис нотификаций.

### 2.3. Общение
* Пользователь -> API Gateway: HTTP3 тк связь не стабильна
* Межсервисное взаимодействие:
    * С Notification Service через RabbitMQ для отложенных сообщений и постепенной обработки.
    * C Video Processing Service через Kafka для гарантии доставки и постепенной обработки.
    * Остальные сервисы общаются по grpc (HTTP2).
    * Инфраструктурные сервисы - по своим протоколам.

### 2.4. Масштабируемость и высокая доступность

1. Масштабирование - в системе по обработке, просмотру видео и сбору их статистики я применил оба алгоритма:
    1. У нас очень много IO-bound операций поддающихся параллелизму, поэтому создаем несколько контейнеров одного 
    приложения, а также выбрать горизонтальное масштабирование основным.
    2. Вместе с этим, у нас есть сервис обработки видео, который хорошо вертикально масштабируется, так как 
    там очень много зависит от мощности и количества ядер CPU, GPU и от объема RAM.
    3. Также при выборе между горизонтальным и вертикальным масштабированием, ориентируемся на данные по нагрузке,
    CPU и остальных компонентов: если помогают новые инстансы - добавляем их, иначе увеличиваем мощности.
        * Ориентируемся на графики CPU, GPU и ошибок по таймауту в графане:
        * Если большая нагрузка на CPU, GPU - накидываем ресурсов.
        * Если только таймауты, то увеличиваем число инстансов.
2. Геораспределенность CDN и S3 - помогает снизить нагрузку на основные сервера.
3. Высокая доступность - при отказе одного из инстансов приложений и/или нод CDN, и/или БД пользовательские запросы переводятся на любой другой, если нагрузка слишком большая - добавляем новые инстансы.
4. Также для высокой доступности стоит настроить метрики и алерты в графане, чтобы моментально реагировать на аварии.
5. Используем Rate Limiter для избежания дудосинга.
6. Используем Circuit breaker для избежания само дудосинга.


## 3. Выбор технологий

### Основная БД - CassandraDB
Основная причина выбора - легко обеспечить AP (из CAP теоремы).
* Легко добавляется новая нода.
* При отключении любой из нод система продолжает функционировать без задержек.
* Данные имеют копии на нодах в количестве replication_factor (обычно 3, реже 5), но общее число нод можно настроить сильно больше.

### БД для авторизации - PostgreSQL
* Необходимость транзакций и отношений.
* Множество плагинов в том числе для безопасности.
* С JWT токенами подразумевается не большая нагрузка на этот сервис -> меньше надобности в масштабировании, хотя read реплики все равно придется использовать.
* Используем несколько read реплик.

### Очередь на обработку видео - Apache Kafka
* Очень высокая пропускная способность.
* Долговременный лог событий.
* Идемпотентные продюсеры, транзакции, откаты, повторный прогон.

### Очередь для нотификаций - RabbitMQ
* Гибкие шаблоны маршрутизации (direct/topic/fanout).
* Легко настроить отложенные уведомления.
* Низкая задержка.
* Удобные ack/перепоставки.

### Redis для сервисного кэша
* Хранит результаты сложных агрегационных запросов, в основном не персонализированных (например, информация о видео с лайками и комментариями).


## 4. Реализация нефункциональных требований

### 4.1. Безопасность
* Использование TLS
    * HSTS - Принудительный переход с HTTP на HTTPS.
    * Используем только TLS 1.2 и 1.3.
    * Выбираем сильные шифры: AES-256-GCM, ChaCha20-Poly1305.
    * Используем TLS Session Tickets для ускорения реконекта.
* Шифрование данных
    * Хешируем пароли через алгоритм Argon2id
    * Шифруем персональные данные (email, номер телефона и другие) через алгоритм AES-256-GCM. В PG есть возможность делать это нативно, можно вшить в шаблон проектирования репозиторий.
    * Шифруем JWT токены через ассиметричный ECDH-ES.
* Нельзя выдавать пользователям отдельные права, либо добавить ее в роль, либо создавать кастомную роль.
    * Обычный пользователь: нет специальных прав, только идентификация.
    * Модератор комментариев: удаление, редактирование чужих комментариев.
    * Поддержка: восстановление комментариев...

### 4.2. Схема получения авторизационного токена
![Схема получения авторизационного токена](./auth-token-flow.png)

### 4.3. Методы мониторинга и управления системой

Используем:
* ELK для логов.
* Grafana + prometheus для метрик + алертинга.

* Метрики
    * Количество запросов к ресурсам, на которые у пользователя нет прав.
    * Количество пользователей с слишком большим RPS.
* Алерты:
    * При большом количестве долгих запросов.
    * При большом количестве 5xx.
    * При падении сервиса.
    * Алерты при появлении не консистентных данных в конкретных местах, который мы пытались предсказать.


## 5. Риски и компромиссы

* Как писал выше, в системе делается приоритет на AP, так как пользователям не получать самую свежую информацию. Например, лайк может появиться на одной из нод CassandraDB раньше, чем на другой.

* CassandraDB легко масштабируется, но не при пиках нагрузки. Мы можем не угадать с количеством нод и наша система будет вынуждена либо терпеть большие нагрузки с задержками запросов, либо еще больше увеличить нагрузку, добавив новую ноду, которая начнет ребалансировать данные. Поэтому тщательно следим за нагрузкой на бд и добавляем ноды при надобности.

* Использование одного ЦОД -> единая точка катастрофы.
